# General configuration
crossdata-connector.akka.cluster.seed-nodes = ["akka.tcp://CrossdataServerCluster@127.0.0.1:13420"]
crossdata-connector.akka.remote.netty.tcp.hostname = "127.0.0.1"
crossdata-connector.akka.remote.netty.tcp.port = 0
crossdata-connector.config.connector.name = "SparkSQLConnector"
crossdata-connector.config.akka.number.connector-actor = 5

# Spark cluster configuration
spark.serializer       =org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator =com.stratio.deep.serializer.DeepKryoRegistrator

#SPARK CONFIG FOR LOCAL
spark.master           ="local[4]"
spark.jars             = []

#SPARK CONFIG FOR REMOTE
#spark.master           ="spark://[IP]:7077"
#spark.jars             =["[PATH]/stratio-connector-sparksql-0.1.0-SNAPSHOT.jar", "[PATH]/crossdata-common-0.3.0-RC2.jar", "[PATH]/stratio-connector-commons-0.5.0-RC2.jar", "[PATH]/spark-hive_2.10-1.3.1.jar", "[PATH]/guava-14.0.1.jar", "[PATH]/cassandra-driver-core-2.1.5.jar", "[PATH]/cassandra-thrift-2.1.3.jar", "[PATH]/mysql-connector-java-5.1.34.jar", "[PATH]/spark-cassandra-connector_2.10-1.3.0-SNAPSHOT.jar"]

spark.driver.memory = 512M
spark.executor.memory = 512M
spark.cores.max = 4
spark.akka.heartbeat.interval = 5000


spark.home             ="[PATH]"

# Connector custom props
connector.sql-context-type = HiveContext #HBaseContext Or SQLContext
connector.query-executors.size  = 5
connector.query-executors.chunk-size = 1000 #rows
connector.count-approx-timeout = 5 #seconds
connector.async-stoppable = true
#spark.hadoop.hbase.zookeeper.quorum = "[IP]"
