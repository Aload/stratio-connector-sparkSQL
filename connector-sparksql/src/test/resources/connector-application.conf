# General configuration
crossdata-connector.akka.cluster.seed-nodes = ["akka.tcp://CrossdataServerCluster@127.0.0.1:13420"]
crossdata-connector.akka.remote.netty.tcp.hostname = "127.0.0.1"
crossdata-connector.akka.remote.netty.tcp.port = 0
crossdata-connector.config.connector.name = "SparkSQLConnector"
crossdata-connector.config.akka.number.connector-actor = 5

# Spark cluster configuration
spark.serializer       =org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator =com.stratio.deep.serializer.DeepKryoRegistrator

#SPARK CONFIG FOR LOCAL
spark.master           ="local[4]"
#spark.home             =/opt/spark-1.3.0-bin-hadoop2.4
#spark.jars             =[]
# SPARK CONFIG FOR MESOS SAMPLE ¡¡¡¡WARNING!!!! REVISE & UPDATE JARS DEPENDENCES
#spark.master           ="mesos://zk://QA-Nodo3-U13.stratio.com:2181,QA-Nodo4-U13.stratio.com:2181,QA-Nodo2-U13.stratio.com:2181/mesos"
#spark.home             =/opt/sds/spark
#spark.jars             =["/opt/sds/connectors/deep/lib/stratio-connector-sparksql-0.X.X.jar","/opt/sds/connectors/deep/lib/crossdata-common-0.X.X.jar"]
spark.driver.memory = 2G
spark.executor.memory = 4G
spark.task.cpus = 3
spark.driver.allowMultipleContexts = true

# Connector custom props
connector.sql-context-type = HiveContext #Or SQLContext
connector.query-executors.size  = 5
connector.query-executors.chunk-size = 1000 #rows
connector.provider = parquet
connector.count-approx-timeout = 5 #seconds
connector.async-stoppable = true